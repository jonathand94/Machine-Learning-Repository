{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SELECCIÓN DE MODELOS I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importando las librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from random import shuffle\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from linear_utils import LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analizando los datos de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargando los datos de entrenamiento\n",
    "\n",
    "Los datos corresponden a ventas en Ames, Iowa. Se obtuvieron de Kaggle: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Indicamos la dirección y nombre del archivo\n",
    "file_name_train = 'Data/train.csv'\n",
    "file_name_test = 'Data/test.csv'\n",
    "train_data = pd.read_csv(file_name_train)\n",
    "test_data = pd.read_csv(file_name_test)\n",
    "\n",
    "# Visualizamos el header de entrenamiento\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos interesa primero ver un modelo de regresión en 2 dimensiones por lo que se hará una prueba solo con el área del primer piso y el precio de la casa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se cargan datos de entrenamiento\n",
    "x_pd = train_data['1stFlrSF']\n",
    "y_pd = train_data['SalePrice']\n",
    "\n",
    "x = x_pd.values.tolist()\n",
    "y = y_pd.values.tolist()\n",
    "\n",
    "x = np.array(x, dtype='float64')\n",
    "x = x.reshape(x.shape[0], 1)\n",
    "\n",
    "y = np.array(y, dtype='float64')\n",
    "y = y.reshape(y.shape[0], 1)\n",
    "\n",
    "# Se aleatoriza\n",
    "idx = [i for i in range(len(x))]\n",
    "shuffle(idx)\n",
    "x = x[idx] \n",
    "y = y[idx]\n",
    "\n",
    "# Se seccionan los datos de entrenamiento\n",
    "split = 0.7\n",
    "x_train = x[0:round(split*x.shape[0])]\n",
    "x_test = x[round(split*x.shape[0]):]\n",
    "y_train = y[0:round(split*y.shape[0])]\n",
    "y_test = y[round(split*y.shape[0]):]\n",
    "\n",
    "# Creamos nuestro objeto lr\n",
    "lr = LR()\n",
    "\n",
    "# Se normalizan los datos\n",
    "x_norm = lr.norm(x_train)\n",
    "y_norm = lr.norm(y_train)\n",
    "\n",
    "x_train_norm = lr.norm(x_train)\n",
    "y_train_norm = lr.norm(y_train)\n",
    "\n",
    "x_test_norm = lr.norm(x_test)\n",
    "y_test_norm = lr.norm(y_test)\n",
    "\n",
    "print(x_train_norm.shape, x_test_norm.shape, y_train_norm.shape, y_test_norm.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sobreajuste vs subajuste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 LMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añadimos la unidad de sesgo\n",
    "x_train_norm_b =  np.insert(x_train_norm, 0, 1, axis=1)\n",
    "x_train_norm_b = x_train_norm_b.T\n",
    "x_test_norm_b =  np.insert(x_test_norm, 0, 1, axis=1)\n",
    "x_test_norm_b = x_test_norm_b.T\n",
    "x_norm_b =  np.insert(x_norm, 0, 1, axis=1)\n",
    "x_norm_b = x_norm_b.T\n",
    "x_train_b =  np.insert(x_train, 0, 1, axis=1)\n",
    "x_train_b = x_train_b.T\n",
    "\n",
    "# Optimizamos con descenso\n",
    "num_iter = 600\n",
    "alpha = 0.5\n",
    "it = np.linspace(0 ,num_iter, num_iter)\n",
    "w_LMS, j_LMS_train, j_LMS_test = lr.fit_gd(x_train_norm_b, y_train_norm, x_test_norm_b, y_test_norm, num_iter, alpha)\n",
    "\n",
    "# Realizamos predicciones del entrenamiento\n",
    "preds_LMS_norm_train = lr.h(w_LMS, x_train_norm_b)\n",
    "preds_LMS_train = lr.denormalize(preds_LMS_norm_train, min(y_train), max(y_train))\n",
    "\n",
    "# Realizamos predicciones en validación\n",
    "preds_LMS_norm_test = lr.h(w_LMS, x_test_norm_b)\n",
    "preds_LMS_test = lr.denormalize(preds_LMS_norm_test, min(y_test), max(y_test))\n",
    "\n",
    "# Graficamos entrenamiento\n",
    "plt.figure()\n",
    "plt.scatter(x_train, y_train, alpha = 1)\n",
    "plt.plot(x_train, preds_LMS_train.T, 'r', linewidth=3)\n",
    "plt.title('Casas en Iowa')\n",
    "plt.xlabel('Área del primer piso [m2]')\n",
    "plt.ylabel('Precio de venta [$]')\n",
    "\n",
    "# Graficamos validación\n",
    "plt.figure()\n",
    "plt.scatter(x_test, y_test, alpha = 1)\n",
    "plt.plot(x_test, preds_LMS_test.T, 'g', linewidth=3)\n",
    "plt.title('Casas en Iowa')\n",
    "plt.xlabel('Área del primer piso [m2]')\n",
    "plt.ylabel('Precio de venta [$]')\n",
    "\n",
    "# Graficamos el costo\n",
    "plt.figure()\n",
    "plt.plot(it, j_LMS_train, 'r', linewidth=3, label='Costo entrenamiento')\n",
    "plt.plot(it, j_LMS_test, 'g', linewidth=3, label='Costo validación')\n",
    "plt.title('Costo cost(w) en el entrenamiento')\n",
    "plt.xlabel('Costo cost(w)')\n",
    "plt.ylabel('Iteraciones')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 NE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizamos con NE\n",
    "w_NE = lr.fit_ne(x_train_norm_b.T, y_train_norm)\n",
    "\n",
    "# Realizamos predicciones del entrenamiento\n",
    "preds_NE_norm_train = lr.h(w_NE, x_train_norm_b)\n",
    "preds_NE_train = lr.denormalize(preds_NE_norm_train, min(y_train), max(y_train))\n",
    "\n",
    "# Realizamos predicciones en validación\n",
    "preds_NE_norm_test = lr.h(w_NE, x_test_norm_b)\n",
    "preds_NE_test = lr.denormalize(preds_NE_norm_test, min(y_test), max(y_test))\n",
    "\n",
    "# Graficamos entrenamiento\n",
    "plt.figure()\n",
    "plt.scatter(x_train, y_train, alpha = 1)\n",
    "plt.plot(x_train, preds_NE_train.T, 'r', linewidth=3)\n",
    "plt.title('Casas en Iowa')\n",
    "plt.xlabel('Área del primer piso [m2]')\n",
    "plt.ylabel('Precio de venta [$]')\n",
    "\n",
    "# Graficamos validación\n",
    "plt.figure()\n",
    "plt.scatter(x_test, y_test, alpha = 1)\n",
    "plt.plot(x_test, preds_NE_test.T, 'g', linewidth=3)\n",
    "plt.title('Casas en Iowa')\n",
    "plt.xlabel('Área del primer piso [m2]')\n",
    "plt.ylabel('Precio de venta [$]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Regresión lineal ponderada\n",
    "Se sabe entonces que el error medio cuadrático se escribe de la siguiente forma:\n",
    "\n",
    "$$J(w)=\\frac{1}{2m}\\sum_{i=1}^{m}\\theta_{i}(h(x)^{(i)}-y^{(i)})^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea entonces la función de ponderación está dada por:\n",
    "\n",
    "$$\\theta_{i} = e^{-\\frac{(x^{(i)}-x)^2}{2\\tau^{2}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de ponderacion\n",
    "def theta(x_i, x, tau=1):\n",
    "    \"\"\"\n",
    "        A weighting bell function that computes the level of importance of a single training input x_i with respect to the specified pivot x.\n",
    "    \n",
    "        params:\n",
    "            x_i: [np_array] an input vector with n feature variables with dimensions (nx1)\n",
    "            x: [np_array] an input vector with n feature variables with dimensions (nx1), which we assigned as our pivot evaluation.\n",
    "            tau: [double] a scalar representing the bandwith of the weighting function\n",
    "                \n",
    "        return:\n",
    "            theta: [np_array] the final weight assigned to this particular fit\n",
    "    \"\"\"\n",
    "    a = -(x_i - x).T.dot((x_i-x))/(2*tau**2)\n",
    "    return np.exp(a)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two input vectors x_i and x\n",
    "x_i = np.array([1, 2], dtype='float64')\n",
    "x_1 = np.array([2, 5], dtype='float64')\n",
    "tau = 2\n",
    "w = theta(x_i, x_1, tau)\n",
    "print(x_i.shape, x_1.shape, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creando la función de costo ponderada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de costo ponderada\n",
    "def J_theta(w, x, y, x_pivot, tau):\n",
    "    \"\"\"\n",
    "        params:\n",
    "            w: [np_array] a vector of weights with dimensions (nx1), where n represents the number of weights.\n",
    "            x: [np_array] a vector of feature variables with dimensions (nxm), \n",
    "                where n represents the number of feature variables and m the number of training examples\n",
    "            y: [np_array] a vector of feature variables with dimensions (mx1), \n",
    "                where m represents the number of target variables\n",
    "            x_pivot: [np_array] the pivot where the cost function will be computed of dimensions (nx1) \n",
    "            tau: [double] bandwith of weighting function\n",
    "        returns:\n",
    "            cost: [double] the mean squared error\n",
    "    \"\"\"\n",
    "    # Get number o training examples \n",
    "    m = x.shape[1]\n",
    "    \n",
    "    # Initialize error term\n",
    "    e = 0\n",
    "    \n",
    "    # Iterate over m training examples\n",
    "    for i in range(m):\n",
    "        \n",
    "        # Get single coordinates\n",
    "        x_i = np.array(x.T[i])\n",
    "        y_i = y[i]\n",
    "                    \n",
    "        # Compute weighted error \n",
    "        th = theta(x_i.T, x_pivot, tau)\n",
    "        e_i = th*(w.T.dot(x_i.T) - y_i)*(w.T.dot(x_i.T) - y_i)\n",
    "\n",
    "        # Accumulate error\n",
    "        e = e + e_i\n",
    "    \n",
    "    return (1/(2*x.shape[1]))*e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a probar la función de costo\n",
    "\n",
    "# Generamos datos aleatoriamente\n",
    "X = 2*np.random.rand(100,1)\n",
    "Y = 4 + 3*X**2 + 5*X**3 + np.random.randn(100,1)\n",
    "\n",
    "# Añadimos la unidad de sesgo\n",
    "X_b =  np.insert(X, 0, 1, axis=1)\n",
    "X_b = np.transpose(X_b)\n",
    "\n",
    "# Inicializamos los pesos aleatoriamente (opcional)\n",
    "w = np.array(np.random.randn(X_b.shape[0], 1))\n",
    "\n",
    "# se calcula la hipótesis para un solo dato pivote\n",
    "x_pivot = np.array([1, 1.5], dtype='float64')\n",
    "tau = 0.01\n",
    "mse = J_theta(w, X_b, Y, x_pivot, tau)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculando el gradiente\n",
    "Recordando que el gradiente está dado por:\n",
    "\n",
    "$$\\bigtriangledown _{w} cost = \\frac{1}{m} \\sum_{i=1}^{m} \\theta^{(i)}(y^{(i)}-h(x^{(i)}))x^{(i)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradiente de la función de costo\n",
    "def dJ_theta(w, x, y, x_pivot, tau):\n",
    "    \"\"\"\n",
    "        params:\n",
    "            w: [np_array] a vector of weights with dimensions (nx1), where n represents the number of weights.\n",
    "            x: [np_array] a vector of feature variables with dimensions (nxm), \n",
    "                where n represents the number of feature variables and m the number of training examples\n",
    "            y: [np_array] a vector of feature variables with dimensions (mx1), \n",
    "                where m represents the number of target variables\n",
    "        returns:\n",
    "            dJ: [double] the derivative of the mean squared error\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get number o training examples \n",
    "    m = x.shape[1]\n",
    "    \n",
    "    # Initialize error term\n",
    "    e = 0\n",
    "    \n",
    "    # Iterate over m training examples\n",
    "    for i in range(m):\n",
    "        \n",
    "        # Get single coordinates\n",
    "        x_i = x.T[i]\n",
    "        y_i = y[i]\n",
    "        \n",
    "        # Compute weighted error \n",
    "        th = theta(x_i, x_pivot, tau)\n",
    "        e_i = (w.T.dot(x_i.T)-y_i)*th*x_i\n",
    "                    \n",
    "        # Accumulate error\n",
    "        e = e + e_i\n",
    "    \n",
    "    return np.array([(1/(x.shape[1]))*e], dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a probar la función de gradiente\n",
    "\n",
    "# Inicializamos los pesos aleatoriamente (opcional)\n",
    "w = np.random.randn(X_b.shape[0], 1)\n",
    "\n",
    "# se calcula la hipótesis para un solo dato pivote\n",
    "x_pivot = np.array([1, 1.5], dtype='float64')\n",
    "\n",
    "deriv_J = dJ_theta(w, X_b, Y, x_pivot, tau=2)\n",
    "print(deriv_J.shape, w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculando el descenso por gradiente para un solo punto pivote\n",
    "\n",
    "$$ w:=w- \\alpha \\bigtriangledown _{w} cost $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizar_LRP(x, y, num_iter, alpha, x_pivot, tau, w = None):\n",
    "    \"\"\"\n",
    "    We calculate gradient descent for minimizing the weighted MSE to obtain the best linear hypothesis.\n",
    "        params:\n",
    "            x: [np_array] a vector of feature variables with dimensions (nxm), \n",
    "                where n represents the number of feature variables and m the number of training examples\n",
    "            y: [np_array] a vector of feature variables with dimensions (mx1), \n",
    "                where m represents the number of target variables\n",
    "            num_iter: [int] an integer indicating the number of iterations of the Gradient Descent algorithm\n",
    "            alpha: [double] learning rate constant specifying the magnitude update step\n",
    "            w: [np_array] vector that contains the initial weights to start optimzing the model with dimensions (n x 1)\n",
    "                \n",
    "        return:\n",
    "            j: [np_array] a vector (num_iter x 1) containing all cost function evaluations during training\n",
    "            w: [np_array] a vector of the final optimized weights with dimensions (nx1)\n",
    "    \"\"\"\n",
    "    \n",
    "    if w is None:\n",
    "        # Inicializamos los pesos aleatoriamente\n",
    "        w = np.random.randn(x.shape[0], 1)\n",
    "\n",
    "    # se generan los vectores\n",
    "    it = np.arange(0, num_iter)\n",
    "    j = np.zeros(num_iter)\n",
    "    \n",
    "    # Se optimiza el modelo por el numero de iteraciones\n",
    "    for i in range(num_iter):\n",
    "\n",
    "        # Calculamos la hipótesis\n",
    "        preds = w.T.dot(x)\n",
    "\n",
    "        # Actualizamos los pesos\n",
    "        w = w - alpha * dJ_theta(w, x, y, x_pivot, tau).T\n",
    "\n",
    "        # Guardamos el costo\n",
    "        j[i] = J_theta(w, x, y, x_pivot, tau)\n",
    "\n",
    "    return w, j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo modelo simple: un pivote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos datos aleatoriamente\n",
    "X = 2*np.random.rand(100,1)\n",
    "X = np.array(X, dtype='float64')\n",
    "Y = 4 + 2*X + 3*X**2 + 5*X**3 + np.random.randn(100,1)\n",
    "Y = np.array(Y, dtype='float64')\n",
    "\n",
    "# Añadimos la unidad de sesgo\n",
    "X_b =  np.insert(X, 0, 1, axis=1)\n",
    "X_b = np.transpose(X_b)\n",
    "\n",
    "# Inicializamos los pesos aleatoriamente (opcional)\n",
    "w = np.random.randn(X.shape[0], 1)\n",
    "\n",
    "# Se gráfican los datos\n",
    "plt.figure()\n",
    "plt.scatter(X, Y, alpha = 1)\n",
    "plt.title('Random data')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "\n",
    "# Se corre el algoritmo para un solo dato pivote\n",
    "x_pivot = np.array([1, 1], dtype='float64')\n",
    "num_iter = 2500\n",
    "alpha = 0.1\n",
    "tau = 0.01\n",
    "w, j = optimizar_LRP(X_b, Y, num_iter, alpha, x_pivot, tau)\n",
    "print(w.shape)\n",
    "\n",
    "# Graficamos la hipotesis lineal\n",
    "preds = w.T.dot(X_b)\n",
    "plt.plot(X, preds.T, 'r')\n",
    "\n",
    "# Graficamos en el punto pivote\n",
    "preds =  w.T.dot(x_pivot)\n",
    "plt.scatter(x_pivot[1], preds, s=100)\n",
    "\n",
    "# Graficamos el costo\n",
    "it = np.linspace(0, num_iter, num_iter)\n",
    "plt.figure()\n",
    "plt.title('Gráfica del costo en el tiempo')\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.ylabel('Costo') \n",
    "plt.plot(it, j)\n",
    "plt.show()\n",
    "\n",
    "print(j[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo modelo simple: varios pivotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_LRP(X_b, Y, num_iter, alpha, pivots, tau):\n",
    "    \n",
    "    \"\"\"\n",
    "        Fit the data in all pivots using the weighting linear regression cost function.     \n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize predictions vector\n",
    "    preds = []\n",
    "    \n",
    "    # Iterate in pivots\n",
    "    for pivot in pivots:\n",
    "        print('Estimando en x = {0}'.format(pivot))\n",
    "        x_pivot = np.array([1, pivot], dtype='float64')\n",
    "        w, j = optimizar_LRP(X_b, Y, num_iter, alpha, x_pivot, tau)\n",
    "\n",
    "        # Graficamos en el punto pivote\n",
    "        pred = w.T.dot(x_pivot)\n",
    "        preds.append(pred)\n",
    "    return(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se corre el algoritmo para 10 datos pivotes\n",
    "pivots = np.linspace(0, 2, 10, dtype='float64')\n",
    "num_iter = 2500\n",
    "alpha = 0.1\n",
    "tau = 0.1\n",
    "\n",
    "preds = predict_LRP(X_b, Y, num_iter, alpha, pivots, tau)\n",
    "\n",
    "# Se gráfican los datos\n",
    "plt.figure()\n",
    "plt.scatter(X, Y, alpha = 1)\n",
    "plt.title('Random data')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    " \n",
    "# Graficamos la curva obtenida\n",
    "plt.scatter(pivots, preds, s=100, c='r')\n",
    "plt.plot(pivots, preds, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo modelo simple: variando el ancho de banda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se gráfican los datos\n",
    "plt.figure()\n",
    "plt.scatter(X, Y, alpha = 1)\n",
    "plt.title('Random data')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "\n",
    "# Se corre el algoritmo para 5 datos pivotes\n",
    "pivots = np.linspace(0, 2, 5, dtype='float64')\n",
    "num_iter = 1500\n",
    "alpha = 0.1\n",
    "tau = 0.01\n",
    "\n",
    "# Vector de taus\n",
    "taus = np.linspace(0.1, 1, 5, dtype='float64')\n",
    "\n",
    "for tau in taus:\n",
    "    print('TAU {0}'.format(tau))\n",
    "    preds = predict_LRP(X_b, Y, num_iter, alpha, pivots, tau)\n",
    "    plt.plot(pivots, preds, label=tau)\n",
    "    print('\\n')\n",
    "    \n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo casas: predecir un pivote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficamos los datos\n",
    "plt.figure()\n",
    "plt.scatter(x_train, y_train, alpha = 1)\n",
    "plt.title('Casas en Iowa')\n",
    "plt.xlabel('Área del primer piso [m2]')\n",
    "plt.ylabel('Precio de venta [$]')\n",
    "\n",
    "# Calculando el pivote\n",
    "x_pivot = np.array([1, 1000], dtype='float64') \n",
    "x_pivot_norm = (x_pivot - min(x_train))/(max(x_train) - min(x_train))\n",
    "\n",
    "# Se corre el algoritmo para un solo dato pivote\n",
    "num_iter = 1000\n",
    "alpha = 0.1\n",
    "tau = 0.1\n",
    "w, j = optimizar_LRP(x_train_norm_b, y_train_norm, num_iter, alpha, x_pivot_norm, tau)\n",
    "\n",
    "# Graficamos en el punto pivote denormalizado\n",
    "preds_norm = w.T.dot(x_pivot_norm)\n",
    "preds = lr.denormalize(preds_norm, min(y_train), max(y_train))\n",
    "plt.scatter(x_pivot[1], preds, s=100)\n",
    "\n",
    "# Graficamos el costo\n",
    "it = np.linspace(0, num_iter, num_iter)\n",
    "plt.figure()\n",
    "plt.title('Gráfica del costo en el tiempo')\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.ylabel('Costo') \n",
    "plt.plot(it, j)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo casas: predecir varios pivotes datos de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficamos los datos\n",
    "plt.figure()\n",
    "plt.scatter(x_train, y_train, alpha = 1)\n",
    "plt.title('Casas en Iowa')\n",
    "plt.xlabel('Área del primer piso [m2]')\n",
    "plt.ylabel('Precio de venta [$]')\n",
    "\n",
    "# se calculan los pivotes\n",
    "pivots = np.linspace(500, 2000, 10, dtype='float64') \n",
    "pivots_norm = (pivots - min(x_train))/(max(x_train) - min(x_train))\n",
    "\n",
    "# Se corre el algoritmo para 10 datos pivotes\n",
    "num_iter = 1000\n",
    "alpha = 0.1\n",
    "tau = 0.1\n",
    "\n",
    "# Vector de taus\n",
    "preds_norm = predict_LRP(x_train_norm_b, y_train_norm, num_iter, alpha, pivots_norm, tau)\n",
    "preds = lr.denormalize(preds_norm, min(y_train), max(y_train))\n",
    "\n",
    "# Se grafican las estimaciones\n",
    "plt.scatter(pivots, preds, s=100, c='r')\n",
    "plt.plot(pivots, preds, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo casas: predecir varios pivotes datos de validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficamos los datos de validación\n",
    "plt.figure()\n",
    "plt.scatter(x_test, y_test, alpha = 1)\n",
    "plt.title('Casas en Iowa')\n",
    "plt.xlabel('Área del primer piso [m2]')\n",
    "plt.ylabel('Precio de venta [$]')\n",
    "\n",
    "# Se grafican las estimaciones\n",
    "plt.scatter(pivots, preds, s=100, c='y')\n",
    "plt.plot(pivots, preds, 'y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo casas: sin normalizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficamos los datos\n",
    "plt.figure()\n",
    "plt.scatter(x_train, y_train, alpha = 1)\n",
    "plt.title('Casas en Iowa')\n",
    "plt.xlabel('Área del primer piso [m2]')\n",
    "plt.ylabel('Precio de venta [$]')\n",
    "\n",
    "# Se corre el algoritmo para 10 datos pivotes\n",
    "pivots = np.linspace(500, 2000, 10, dtype='float64')\n",
    "num_iter = 800\n",
    "alpha = 0.1\n",
    "tau = 0.1\n",
    "\n",
    "# Vector de taus\n",
    "preds = predict_LRP(x_train_b, y_train, num_iter, alpha, pivots, tau)\n",
    "\n",
    "# Se grafican las estimaciones\n",
    "plt.scatter(pivots, preds, s=100, c='r')\n",
    "plt.plot(pivots, preds, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo casas: variando el ancho de banda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se gráfican los datos\n",
    "plt.figure()\n",
    "plt.scatter(x_train, y_train, alpha = 1)\n",
    "plt.title('Casas en Iowa')\n",
    "plt.xlabel('Área del primer piso [m2]')\n",
    "plt.ylabel('Precio de venta [$]')\n",
    "\n",
    "# Se corre el algoritmo para 5 datos pivotes\n",
    "pivots = np.linspace(500, 2000, 10, dtype='float64') \n",
    "pivots_norm = (pivots - min(x_train))/(max(x_train) - min(x_train))\n",
    "num_iter = 800\n",
    "alpha = 0.1\n",
    "\n",
    "# Vector de taus\n",
    "taus = np.linspace(0.1, 1, 5, dtype='float64')\n",
    "\n",
    "for tau in taus:\n",
    "    print('TAU {0}'.format(tau))\n",
    "    preds_norm = predict_LRP(x_train_norm_b, y_train_norm, num_iter, alpha, pivots_norm, tau)\n",
    "    preds = lr.denormalize(preds_norm, min(y_train), max(y_train))\n",
    "    plt.plot(pivots, preds, label=tau)\n",
    "    print('\\n')\n",
    "    \n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Regularización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularización L2: Descenso por Gradiente\n",
    "\n",
    "La función de costo $cost(w)$ con el término de regularización L2 está dada por:\n",
    "\n",
    "$$cost(w)=(\\vec{w}^{T}\\phi(X)-\\vec{y})^{T}(\\vec{w}^{T}\\phi(X)-\\vec{y})+ \\frac{\\lambda}{2} \\vec{w}^{T}{\\vec{w}}$$\n",
    "\n",
    "Se puede calcular el gradiente de la función, lo que se define como:\n",
    "\n",
    "$$\\nabla_{w} cost(w) = \\frac{1}{m} X^{T}(X \\vec{w}-\\vec{y}) + \\lambda\\vec{w}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularización L2: Ecuaciones Normales\n",
    "\n",
    "Igualando el gradiente $\\nabla _{w} cost(w)$ a cero y despejando para $\\vec{w}$ se obtienen las siguientes ecuaciones:\n",
    "\n",
    "$$\\vec{w} = (X^{T}X + \\lambda I)^{-1} X^{T}y$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programamos la función de gradiente\n",
    "$$\\nabla_{w} cost(w) = \\frac{1}{m} X^{T}(X \\vec{w}-\\vec{y}) + \\lambda\\vec{w}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def dJ(w, phi, y, l2 = 0):\n",
    "        \"\"\"\n",
    "            params:\n",
    "                w: [np_array] a vector of weights with dimensions (nx1), where n represents the number of weights.\n",
    "                x: [np_array] a vector of feature variables with dimensions (nxm),\n",
    "                    where n represents the number of feature variables and m the number of training examples\n",
    "                y: [np_array] a vector of feature variables with dimensions (mx1),\n",
    "                    where m represents the number of target variables\n",
    "                l2: [double] L2 regularization constant \n",
    "                \n",
    "            returns:\n",
    "                dJ: [double] the derivative of the mean squared error\n",
    "        \"\"\"\n",
    "        e = y.T - w.T.dot(phi.T)\n",
    "        return (1 / (phi.shape[0])) * phi.T.dot(e.T) + l2*w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función de costo:\n",
    "\n",
    "$$cost(w)=(\\vec{w}^{T}\\phi(X)-\\vec{y})^{T}(\\vec{w}^{T}\\phi(X)-\\vec{y})+ \\frac{\\lambda}{2} \\vec{w}^{T}{\\vec{w}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(w, phi, y, l2=0):\n",
    "    \"\"\"\n",
    "        params:\n",
    "            w: [np_array] a vector of weights with dimensions (nx1), where n represents the number of weights.\n",
    "            x: [np_array] a vector of feature variables with dimensions (nxm),\n",
    "                where n represents the number of feature variables and m the number of training examples\n",
    "            y: [np_array] a vector of feature variables with dimensions (mx1),\n",
    "                where m represents the number of target variables\n",
    "        returns:\n",
    "            cost: [double] the mean squared error\n",
    "    \"\"\"\n",
    "    e = y.T - w.T.dot(phi.T)\n",
    "    return (1/(2*phi.shape[1])) * (np.sum(np.square(e))) + (l2/2)*w.T.dot(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función de optimización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def optimizar_LMS(x_train, y_train, x_test, y_test, num_iter, alpha, w=None, l2=0):\n",
    "        \"\"\"\n",
    "        We calculate gradient descent for minimizing the MSE to obtain the best linear hypothesis.\n",
    "            params:\n",
    "                x: [np_array] a vector of feature variables with dimensions (nxm),\n",
    "                    where n represents the number of feature variables and m the number of training examples\n",
    "                y: [np_array] a vector of feature variables with dimensions (mx1),\n",
    "                    where m represents the number of target variables\n",
    "                num_iter: [int] an integer indicating the number of iterations of the Gradient Descent algorithm\n",
    "                alpha: [double] learning rate constant specifying the magnitude update step\n",
    "                w: [np_array] vector that contains the initial weights to start optimzing the model with dimensions (n x 1)\n",
    "                l2: [double] L2 regularization constant \n",
    "                \n",
    "            return:\n",
    "                j: [np_array] a vector (num_iter x 1) containing all cost function evaluations during training\n",
    "                w: [np_array] a vector of the final optimized weights with dimensions (nx1)\n",
    "        \"\"\"\n",
    "\n",
    "        if w is None:\n",
    "            # Inicializamos los pesos aleatoriamente\n",
    "            w = np.random.randn(x_train.shape[0], 1)\n",
    "\n",
    "        # se generan los vectores\n",
    "        j_train = np.zeros(num_iter)\n",
    "        j_test = np.zeros(num_iter)\n",
    "\n",
    "        # Se optimiza el modelo por el numero de iteraciones\n",
    "        for i in range(num_iter):\n",
    "\n",
    "            # Actualizamos los pesos\n",
    "            w = w + alpha * dJ(w, x_train, y_train, l2=l2)\n",
    "\n",
    "            # Guardamos el costo\n",
    "            j_train[i] = cost(w, x_train, y_train, l2=l2)\n",
    "\n",
    "            # Guardamos el costo\n",
    "            j_test[i] = cost(w, x_test, y_test, l2=l2)\n",
    "\n",
    "        return w, j_train, j_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "# Generamos datos aleatoriamente\n",
    "m = 50\n",
    "noise = np.array([1*np.random.uniform(size=m)]).T\n",
    "\n",
    "X = np.array([2*np.linspace(-20, 20, m)]).T\n",
    "Y = -40 - 3*X - 5*X**2 + 10*X**3 + 2*X**7 + noise\n",
    "\n",
    "# Aleatorizamos\n",
    "# Se aleatoriza\n",
    "idx = [i for i in range(len(X))]\n",
    "shuffle(idx)\n",
    "X = X[idx] \n",
    "Y = Y[idx]\n",
    "\n",
    "# Normalizamos\n",
    "X_norm = lr.norm(X)\n",
    "Y_norm = lr.norm(Y)\n",
    "\n",
    "# Dividimos\n",
    "split = 0.7\n",
    "x_train_norm = X_norm[0:round(split*X_norm.shape[0])]\n",
    "x_test_norm = X_norm[round(split*X_norm.shape[0]):]\n",
    "y_train_norm = Y_norm[0:round(split*Y_norm.shape[0])]\n",
    "y_test_norm = Y_norm[round(split*Y_norm.shape[0]):]\n",
    "\n",
    "x_train = X[0:round(split*X.shape[0])]\n",
    "x_test = X[round(split*X.shape[0]):]\n",
    "y_train = Y[0:round(split*Y.shape[0])]\n",
    "y_test = Y[round(split*Y.shape[0]):]\n",
    "\n",
    "# Se corre el algoritmo para K lambdas\n",
    "num_lambdas = 4\n",
    "lambdas = np.linspace(0, 0.5, num_lambdas, dtype='float64') \n",
    "num_iter = 300\n",
    "alpha = 0.01\n",
    "it = np.linspace(0 ,num_iter, num_iter)\n",
    "degree = 10\n",
    "\n",
    "# Basis functions\n",
    "degrees = range(degree + 1)\n",
    "phi_train = lr.expand(x_train_norm, bf=lr.polynomial_basis_function, bf_args=degrees[1:])\n",
    "phi_test = lr.expand(x_test_norm, bf=lr.polynomial_basis_function, bf_args=degrees[1:])\n",
    "w = np.random.randn(phi_train.shape[1], 1)\n",
    "\n",
    "# Graficamos \n",
    "fig, axs = plt.subplots(num_lambdas, 3, figsize=(10, 10))\n",
    "fig.subplots_adjust(hspace=1, wspace=1)\n",
    "\n",
    "for i, l2 in enumerate(lambdas):\n",
    "\n",
    "    w_LMS, j_LMS_train, j_LMS_test = optimizar_LMS(phi_train, y_train_norm, phi_test, y_test_norm, num_iter, alpha, w=w, l2=l2)\n",
    "\n",
    "    # Realizamos predicciones del entrenamiento\n",
    "    preds_train = w_LMS.T.dot(phi_train.T)\n",
    "    preds_train = lr.denormalize(preds_train, min(Y), max(Y))\n",
    "    preds_test = w_LMS.T.dot(phi_test.T)\n",
    "    preds_test = lr.denormalize(preds_test, min(Y), max(Y))\n",
    "\n",
    "    # Realizamos predicciones en validación\n",
    "    axs[i][0].scatter(x_train, y_train, alpha = 1)\n",
    "    axs[i][0].scatter(x_train, preds_train.T, label='Lambda = {0:0.03f}'.format(l2))\n",
    "    axs[i][0].set_title('Casas en Iowa')\n",
    "    axs[i][0].set_xlabel('Área del primer piso [m2]')\n",
    "    axs[i][0].set_ylabel('Precio de venta [$]')\n",
    "    axs[i][0].legend()\n",
    "    \n",
    "    axs[i][1].scatter(x_test, y_test, alpha = 1)\n",
    "    axs[i][1].scatter(x_test, preds_test.T, label='Lambda = {0:0.03f}'.format(l2))\n",
    "    axs[i][1].set_title('Casas en Iowa')\n",
    "    axs[i][1].set_xlabel('Área del primer piso [m2]')\n",
    "    axs[i][1].set_ylabel('Precio de venta [$]')\n",
    "    axs[i][1].legend()\n",
    "\n",
    "    # Graficamos el costo\n",
    "    axs[i][2].plot(it, j_LMS_train, 'r', linewidth=3, label='Costo entrenamiento {0:0.03f}'.format(j_LMS_train[-1]))\n",
    "    axs[i][2].plot(it, j_LMS_test, 'g', linewidth=3, label='Costo validación {0:0.03f}'.format(j_LMS_test[-1]))\n",
    "    axs[i][2].set_title('Costo cost(w) en el entrenamiento')\n",
    "    axs[i][2].set_xlabel('Costo cost(w)')\n",
    "    axs[i][2].set_ylabel('Iteraciones')\n",
    "    axs[i][2].legend()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ecuaciones normales\n",
    "\n",
    "$$\\vec{w} =(\\lambda I + \\phi^{T} \\phi)^{-1} \\phi ^{T} \\vec{y}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ne(phi, y, l2=0):\n",
    "    \"\"\"\n",
    "        params:\n",
    "            x: [np_array] a vector of feature variables with dimensions (mxn), \n",
    "                where n represents the number of feature variables and m the number of training examples\n",
    "            y: [np_array] a vector of feature variables with dimensions (nx1), \n",
    "                where m represents the number of target variables\n",
    "                \n",
    "        return:\n",
    "            w: [np_array] a vector of the final optimized weights with dimensions (nx1)\n",
    "    \"\"\"\n",
    "    return np.linalg.inv(l2*np.identity(phi.shape[1]) + phi.T.dot(phi)).dot(phi.T).dot(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se corre el algoritmo para K lambdas\n",
    "num_lambdas = 5\n",
    "lambdas = np.linspace(0, 0.4, num_lambdas, dtype='float64') \n",
    "it = np.linspace(0 ,num_iter, num_iter)\n",
    "degree = 7\n",
    "\n",
    "# Basis functions\n",
    "degrees = range(degree + 1)\n",
    "phi_train = lr.expand(x_train_norm, bf=lr.polynomial_basis_function, bf_args=degrees[1:])\n",
    "phi_test = lr.expand(x_test_norm, bf=lr.polynomial_basis_function, bf_args=degrees[1:])\n",
    "w = np.random.randn(phi_train.shape[1], 1)\n",
    "\n",
    "# Graficamos \n",
    "fig, axs = plt.subplots(num_lambdas, 2, figsize=(10, 10))\n",
    "fig.subplots_adjust(hspace=1, wspace=1)\n",
    "\n",
    "for i, l2 in enumerate(lambdas):\n",
    "\n",
    "    w_LMS = fit_ne(phi_train, y_train_norm, l2=l2)\n",
    "\n",
    "    # Realizamos predicciones del entrenamiento\n",
    "    preds_train = w_LMS.T.dot(phi_train.T)\n",
    "    preds_train = lr.denormalize(preds_train, min(Y), max(Y))\n",
    "    preds_test = w_LMS.T.dot(phi_test.T)\n",
    "    preds_test = lr.denormalize(preds_test, min(Y), max(Y))\n",
    "    \n",
    "    # Calculamos el costo\n",
    "    j_train = cost(w, phi_train, y_train_norm, l2=l2)\n",
    "    j_test = cost(w, phi_test, y_test_norm, l2=l2)\n",
    "\n",
    "    # Realizamos predicciones en validación\n",
    "    axs[i][0].scatter(x_train, y_train, alpha = 1)\n",
    "    axs[i][0].scatter(x_train, preds_train.T, label='Lambda = {0:0.03f}'.format(l2))\n",
    "    axs[i][0].set_title('Casas en Iowa - {0:0.03}'.format(j_train[0][0]))\n",
    "    axs[i][0].set_xlabel('Área del primer piso [m2]')\n",
    "    axs[i][0].set_ylabel('Precio de venta [$]')\n",
    "    axs[i][0].legend()\n",
    "    \n",
    "    axs[i][1].scatter(x_test, y_test, alpha = 1)\n",
    "    axs[i][1].scatter(x_test, preds_test.T, label='Lambda = {0:0.03f}'.format(l2))\n",
    "    axs[i][1].set_title('Casas en Iowa - {0:0.03}'.format(j_test[0][0]))\n",
    "    axs[i][1].set_xlabel('Área del primer piso [m2]')\n",
    "    axs[i][1].set_ylabel('Precio de venta [$]')\n",
    "    axs[i][1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
